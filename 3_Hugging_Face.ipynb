{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Model, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda122.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\bin\\cudart64_12.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 122\n",
      "CUDA SETUP: Loading binary d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda122.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('D:/anaconda/envs/Pray/bin')}\n",
      "  warn(msg)\n",
      "d:\\anaconda\\envs\\Pray\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: D:\\anaconda\\envs\\Pray did not contain ['cudart64_110.dll', 'cudart64_120.dll', 'cudart64_12.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download in https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "df1 = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment(row):\n",
    "    if row['sentiment'] == 'positive':\n",
    "        row['sentiment'] =  1\n",
    "    else:\n",
    "        row['sentiment'] = 0\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:03<00:00, 15296.65it/s]\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.progress_apply(make_sentiment, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_idx, test_idx = train_test_split(df1.index,test_size = 0.2,\n",
    "                                       random_state = 42, stratify = df1['sentiment'])\n",
    "# 8:2 로 train / test split\n",
    "\n",
    "train_idx, val_idx = train_test_split(df1.iloc[train_all_idx].index,test_size = 0.25, random_state = 42, stratify = df1.iloc[train_all_idx]['sentiment'])\n",
    "# 6 : 2 : 2 로 train / valid / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df1.iloc[train_idx].copy()\n",
    "df_test = df1.iloc[test_idx].copy()\n",
    "df_val = df1.iloc[val_idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    15000\n",
       "0    15000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    5000\n",
       "1    5000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    5000\n",
       "0    5000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff= np.array([len(i) for i in tokenizer.batch_encode_plus( df1['review'].values)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(ff)\n",
    "\n",
    "# np.quantile(ff,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = 512\n",
    "\n",
    "# train_sentence = tokenizer([i for i  in df_train['review'].values], add_special_tokens=True, \n",
    "#                                              padding='max_length', truncation=True, max_length=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_sentence = tokenizer([i for i in list(df_val['review'].values)], add_special_tokens=True, \n",
    "#                                              padding='max_length', truncation=True, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence = tokenizer([i for i in list(df_test['review'].values)], add_special_tokens=True, \n",
    "#                                             padding='max_length', truncation=True, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df1.reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentence['label'] = df2.iloc[train_idx].reset_index(drop = True)['sentiment']\n",
    "# val_sentence['label'] = df2.iloc[val_idx].reset_index(drop = True)['sentiment']\n",
    "# test_sentence['label'] = df2.iloc[test_idx].reset_index(drop = True)['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save\n",
    "# import pickle\n",
    "\n",
    "# with open('train_sentence.pickle', 'wb') as f:\n",
    "#     pickle.dump(train_sentence, f, pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# with open('val_sentence.pickle', 'wb') as f:\n",
    "#     pickle.dump(val_sentence, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('test_sentence.pickle', 'wb') as f:\n",
    "#     pickle.dump(test_sentence, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('train_sentence.pickle', 'rb') as f:\n",
    "    train_sentence = pickle.load(f)\n",
    "    \n",
    "# load\n",
    "with open('val_sentence.pickle', 'rb') as f:\n",
    "    val_sentence = pickle.load(f)\n",
    "    \n",
    "# load\n",
    "with open('test_sentence.pickle', 'rb') as f:\n",
    "    test_sentence = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, encoded_data):\n",
    "        self.input_ids = encoded_data['input_ids']\n",
    "        # self.token_type_ids = encoded_data['token_type_ids']\n",
    "        self.attention_mask = encoded_data['attention_mask']\n",
    "        self.label = encoded_data['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            # 'token_type_ids': torch.tensor(self.token_type_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmoDataset(train_sentence)\n",
    "test_dataset = EmoDataset(test_sentence)\n",
    "val_dataset = EmoDataset(val_sentence)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,drop_last=True)  # batch_size는 원하는 값으로 설정하세요.\n",
    "valid_loader = DataLoader(val_dataset, batch_size=64, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train (Fine-tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, model, num_labels=1):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.dropout_2 = nn.Dropout(0.3)\n",
    "        # self.classifier = nn.Linear(768, 3)\n",
    "        # self.classifier_1 = nn.Linear(768, 512)\n",
    "        # self.classifier_2 = nn.Linear(512, 128)\n",
    "        self.classifier_3 = nn.Linear(768, num_labels) # 768은 BERT의 hidden size, num_labels는 분류하려는 클래스 수 (긍정/부정이므로 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "\n",
    "        pooled_output = torch.mean(outputs['last_hidden_state'],dim = 1)\n",
    "        \n",
    "        \n",
    "        output_3 = self.dropout(pooled_output)\n",
    "        logits = self.classifier_3(output_3)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# 모든 매개변수를 리스트로 변환\n",
    "\n",
    "sentiment_model = SentimentClassifier(model).to(device)\n",
    "\n",
    "# 1. 모든 파라미터를 freeze\n",
    "for param in sentiment_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 분류 레이어의 파라미터만 학습되도록 설정\n",
    "\n",
    "for param in sentiment_model.classifier_3.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "\n",
    "# # 3. 마지막 attention 레이어의 파라미터만 학습되도록 설정\n",
    "# for param in sentiment_model.model.encoder.layer[11].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # 3. 마지막 attention 레이어의 파라미터만 학습되도록 설정\n",
    "# for param in sentiment_model.kobert.encoder.layer[10].parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.3, inplace=False)\n",
       "  (classifier_3): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in tqdm(train_loader, desc=\"Processing\", leave =False):\n",
    "#     input_ids = batch['input_ids'].to(device)\n",
    "#     # token_type_ids = batch['token_type_ids'].to(device)\n",
    "#     attention_mask = batch['attention_mask'].to(device)\n",
    "#     train_y =  batch['label'].to(device).float()\n",
    "\n",
    "\n",
    "#     outputs = sentiment_model(input_ids=input_ids, attention_mask = attention_mask)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "save_path = 'DistrilBERT_IMDB.pth'\n",
    "def train_model(model, criterion, optimizer, early_stop, epochs, train_loader, valid_loader):\n",
    "    train_losses, train_accuracies, valid_losses, valid_accuracies, lowest_loss, lowest_epoch = list(), list(), list(), list(), np.inf, 0\n",
    "\n",
    "    # DEBUG\n",
    "    # progress_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy, train_corrects, valid_loss, valid_accuracy, valid_corrects = 0, 0, 0, 0, 0, 0\n",
    "        train_correct, valid_correct = 0, 0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            # token_type_ids = batch['token_type_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            train_y =  batch['label'].to(device).float()\n",
    "\n",
    "\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            \n",
    "            loss = criterion(outputs.reshape(-1), train_y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pred = torch.round(torch.sigmoid(outputs.detach().clone().reshape(-1)))\n",
    "            train_correct += pred.eq(train_y.detach().clone()).sum().item()\n",
    "\n",
    "\n",
    "            # DEBUG\n",
    "            # if (progress_count % 10) == 0:\n",
    "            #    print (y_pred.eq(train_y.detach().cpu()).sum().item(), len(y_pred))\n",
    "            # progress_count += 1\n",
    "\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                # token_type_ids = batch['token_type_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                valid_y = batch['label'].to(device).float()\n",
    "                outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "                loss = criterion(outputs.reshape(-1), valid_y)\n",
    "                valid_loss += loss.item()\n",
    "                pred = torch.round(torch.sigmoid(outputs.detach().clone().reshape(-1)))\n",
    "                valid_correct += pred.eq(valid_y.detach().clone()).sum().item()\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracy = valid_correct / len(valid_loader.dataset)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        elapsed_time = time.time() - start\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}]: {elapsed_time:.3f} sec(elapsed time), train loss: {train_losses[-1]:.4f}, valid loss: {valid_losses[-1]:.4f}, train acc: {train_accuracy * 100:.3f}%, valid acc: {valid_accuracy * 100:.3f}%\")\n",
    "\n",
    "\n",
    "        if valid_losses[-1] < lowest_loss:\n",
    "            lowest_loss = valid_losses[-1]\n",
    "            lowest_epoch = epoch\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "            torch.save(best_model, save_path)\n",
    "        else:\n",
    "            if (early_stop > 0) and lowest_epoch + early_stop < epoch:\n",
    "                print (\"Early Stopped\", epoch, \"epochs\")\n",
    "                break\n",
    "\n",
    "\n",
    "        wandb.log({\"Training loss\" : train_losses[-1], \"Validation loss\" : valid_losses[-1] ,\"Lowest loss\": lowest_loss,\n",
    "                   'Lowest_epoch' : lowest_epoch, 'epoch' : epoch, \n",
    "                   \"Valid_Accuracy\" : 100 *valid_accuracy,\n",
    "                   })\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, lowest_loss, train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import wandb\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "sentiment_model.to(device)\n",
    "optimizer =torch.optim.Adam(sentiment_model.parameters())\n",
    "early_stop = 10\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoon303b\u001b[0m (\u001b[33mku_software\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\USER\\Documents\\Jupyter_Notebook\\Graduate_중앙대\\wandb\\run-20240403_161241-f3m0gnq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ku_software/distilbert-imdb/runs/f3m0gnq3' target=\"_blank\">stellar-river-12</a></strong> to <a href='https://wandb.ai/ku_software/distilbert-imdb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ku_software/distilbert-imdb' target=\"_blank\">https://wandb.ai/ku_software/distilbert-imdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ku_software/distilbert-imdb/runs/f3m0gnq3' target=\"_blank\">https://wandb.ai/ku_software/distilbert-imdb/runs/f3m0gnq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ku_software/distilbert-imdb/runs/f3m0gnq3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x25a2fe07210>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"distilbert-imdb\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": None,\n",
    "    \"architecture\": \"distilbert\",\n",
    "    \"optimizer\" : 'Adam',\n",
    "    'loss' : 'BCEWithLogitsLoss'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [04:25<00:00,  1.76it/s]\n",
      "100%|██████████| 156/156 [01:22<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100]: 348.289 sec(elapsed time), train loss: 0.1756, valid loss: 0.1527, train acc: 93.543%, valid acc: 94.420%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 68/468 [00:39<03:52,  1.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentiment_model_best, lowest_loss, train_losses, valid_losses, train_accuracies, valid_accuracies \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentiment_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, early_stop, epochs, train_loader, valid_loader)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 33\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     35\u001b[0m train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(train_y\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone())\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentiment_model_best, lowest_loss, train_losses, valid_losses, train_accuracies, valid_accuracies =  train_model(\n",
    "    sentiment_model, criterion, optimizer, early_stop, epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.load_state_dict(torch.load('DistrilBERT_IMDB.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:25<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_loss = 0\n",
    "\n",
    "sentiment_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        test_y = batch['label'].to(device).float()\n",
    "        pred = sentiment_model(input_ids,  attention_mask=attention_mask)\n",
    "\n",
    "        loss = criterion(pred.reshape(-1), test_y)\n",
    "        test_loss += loss.item()\n",
    "        y_pred = torch.round(torch.sigmoid(pred.detach().clone().reshape(-1)))\n",
    "        test_correct += y_pred.eq(test_y.detach()).sum().item()\n",
    "\n",
    "        \n",
    "        \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = test_correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9437"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9437"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매우 큰 과적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
